---
title: "ESM223 Week 2"
author: "Sage"
format: html
editor: visual
---

## 

```{r}
install.packages("spData")
```

```{r}
rm(list = ls()) # clears all variable in environment
library(sf) # for handling vector data
library(tmap) # for making maps
library(tidyverse) # because we love the tidyverse
library(spData) # preloaded spatial data


```

Let’s start by looking at how we can construct a `sf` object. Typically we will load `sf` objects by reading in data. However, it can be helpful to see how `sf` objects are created from scratch.

First, we create a geometry for London by supplying a point and coordinate reference system.

```{r}

# create st_point with longitude and latitude for London
# simple feature geometry
london_point <- st_point(c(0.1, 51.5)) #st_ for all sf functions

# add coordinate reference system
# simple feature collection
# crs is a code for the point
london_geom <- st_sfc(london_point, crs = 4326) 

```

Then, we supply some non-geographic attributes by creating a data frame with attributes about London.

```{r}
# create data frame of attributes about London
london_attrib <- data.frame(
  name = "London",
  temperature = 25, # in Celsius
  date = as.Date("2017-06-21")
  )
```

And we attach the simple feature collection and data frame to create a `sf` object. Check out the class of the new object we created.

```{r}
# combine geometry and data frame
# simple feature object
london_sf <- st_sf(london_attrib, geometry = london_geom)

# check class
class(london_sf) # geometry is stored in a cell
```

We can also check out what the CRS looks like:

```{r}
st_crs(london_sf)
```

```{r}
st_crs(london_sf)$IsGeographic
```

```{r}
st_crs(london_sf)$proj4string
```

### **Existing `sf` object**

Now let’s look at an existing `sf` object representing countries of the world:

```{r}
world <- spData::world
class(world)
```

```{r}
dim(world)
```

```{r} 
# dataframe attributes
names(world)
```

We can see that this object contains both spatial data (`geom` column) and attributes about those geometries. We can perform operations on the attribute data, just like we would with a normal data frame.

```{r}

summary(world$lifeExp)

```

The geometry column is “sticky”, meaning it will stick around unless we explicitly get rid of it. For example, `dplyr`’s `select()` function won’t get rid of it.

```{r}
world_df <- world %>%
  select(-geom) #doesn't actually remove the geom column

world_df <- world %>%
  select(continent)


world_df <- st_drop_geometry(world) #remove geom


colnames(world_df) # geom still shows up as a column unless st_drop


```

To drop the `geom` column and convert this `sf` object into a data frame, we need to drop the geometry column using the `st_drop_geometry()`.

```{r}
world_df <- st_drop_geometry(world)
class(world_df)
```

```{r}
names(world_df)
```

```{r}
ncol(world)
```

```{r}
ncol(world_df)
```

In some cases we will be working with data which is represented with different coordinate reference systems (CRS). Whenever we work with multiple spatial data objects, we need to check that the CRSs match.

Let’s create another `sf` object for London, but now represented with a project coordinate system

```{r}
london_proj <- data.frame(x = 530000, y = 180000) %>% # in meters
  st_as_sf(coords = c("x", "y"), crs = "EPSG:27700") # great for reading in data "x" can be "long"
```

We can check the CRS of any data using the `st_crs()` function.

```{r}
st_crs(london_proj)
```

This is a lot of information to read, so if we wanted to use this point with our other London point, we need to check to see if they are using the same CRS.

```{r}
st_crs(london_proj) == st_crs(london_sf)
```

To transform the CRS of a dataset, we use the `st_transform()` function. In the `crs` argument, we need to specify the coordinate reference system. We can do this by either supplying a CRS code or specifying the CRS of another dataset using the `st_crs()` function.

```{r}
# grab the crs from object we want to match
# This one line will get used a bunch
london_sf_transform <- st_transform(london_sf, crs = st_crs(london_proj)) 
```

Now if we check, the CRS between the two datasets should match

```{r}
# build into homework to check - 

if(st_crs(london_sf_transform) == st_crs(london_proj)){
  print("it's a match!") #"coord ref sys of data sys match"
} else {
  error("still not a match") #"crs do not match" spell it out
}

st_crs(london_sf_transform) == st_crs(london_proj)
```

Hopefully we’re already thinking about how we could build checking coordinate reference systems into our workflows.

For example, we could add code like the following that transforms the CRS of `dataset2` to match `dataset1` and prints out a warning message.

```{r}
if(st_crs(dataset1) != st_crs(dataset2)){
  warning("coordinate refrence systems do not match")
  dataset2 <- st_transform(dataset1, crs = st_crs(dataset1))
}
```

### **Changing map projections**

Remember that whenever we make a map we are trying to display three dimensional data with only two dimensions. To display 3D data in 2D, we use projections. Which projection you use can have big implications for how you display information.

To the projection of our data, we could:

-   reproject the underlying data

-   or in `tmap` we can specify the projection we want the map to use

Let’s compare global maps using two different projections:

-   Equal Earth is an equal-area pseudocylindrical projection (EPSG 8857)

-   Mercator is a conformal cylindrical map that preserves angles (EPSG 3395)

```{r}
tm_shape(world, projection = 8857) +
  tm_fill(col = "area_km2")
```

```{r}
tm_shape(world, projection = 3395) +
  tm_fill(col = "area_km2")
```

## **4. Vector attribute subsetting**

Often we’ll want to manipulate `sf` objects in the same ways as we might with tabular data in data frames. The great thing about the simple features data model, is we can largely treat spatial objects the same as data frames.

### **`dplyr` functions!**

This means that we can use all of our favorite `dplyr` functions on `sf` objects – yay!

We can select columns…

```{r}
world %>%
  select(name_long, pop)
```

Or remove columns…

```{r}
world %>%
  select(-subregion, -area_km2)
```

Or select AND rename columns

```{r}
world %>%
  select(name = name_long, population = pop)
```

Or filter observations based on variables

```{r}
world1 <- world %>%
  filter(area_km2 < 10000)

summary(world1$area_km2)
```

```{r}
world2 <- world %>%
  filter(lifeExp >= 80)

nrow(world2)
```

### **Chaining commands with pipes**

Because we can use `dplyr` functions with `sf` objects, we can chain together commands using the pipe operator.

Let’s try to find the country in Asia with the highest life expectancy

```{r}
world %>%   
  filter(continent == "Asia") %>%   
  select(name_long, continent, lifeExp) %>%   
  slice_max(lifeExp) %>% # will pick row with max value
  st_drop_geometry()
```

### **Vector attribute aggregation**

Aggregation is the process of summarizing data with one or more ‘grouping’ variables. For example, using the ‘world’ which provides information on countries of the world, we might want to aggregate to the level of continents. It is important to note that aggregating data *attributes* is a different process from aggregating *geographic* data, which we will cover later.

Let’s try to find the total population within each continent:

```{r}
world %>%
  group_by(continent) %>% # total population for each continent
  summarize(population = sum(pop, na.rm = TRUE)) %>% #add pop
  st_drop_geometry()
```

Let’s also find the total area and number of countries in each continent:

```{r}
world %>%
  group_by(continent) %>% # total pop for each cont
  summarize(population = sum(pop, na.rm = TRUE), # sum for each
            area_km2 = sum(area_km2, na.rm = TRUE),
            n_countries = n()) %>%
  st_drop_geometry()
```

Building on this, let’s find the population density of each continent, find the continents with highest density and arrange by the number of countries. We’ll drop the geometry column to speed things up.

```{r}
world %>%
  st_drop_geometry() %>%
  group_by(continent) %>%
  summarize(population = sum(pop, na.rm = TRUE),
            area_km2 = sum(area_km2, na.rm = TRUE),
            n_countries = n()) %>%
  mutate(density = round(population/area_km2)) %>%
  slice_max(density, n = 3) %>%
  arrange(desc(n_countries))
```

## **5. Joins with vector attributes**

A critical part of many data science workflows is combining data sets based on common attributes. In R, we do this using multiple join functions, which follow SQL conventions.

Let’s start by looking a data set on national coffee production from the `spData` package:

```{r}
coffee_data <- spData::coffee_data
head(coffee_data)

nrow(coffee_data)
nrow(world)

```

It appears that `coffee_data` contains information on the amount of coffee produced in 2016 and 2017 from a subset of countries.

```{r}
nrow(coffee_data)
```

```{r}
nrow(world)
```
 
```{r}

world_coffee <- left_join(world, coffee_data, by = "name_long")

coffee_world <- left_join(coffee_data, world, by = "name_long")

```

```{r}
tm_shape(world_coffee) +
  tm_fill(col = "coffee_production_2017")
```
```{r}
world_coffee_inner <- inner_join(world, coffee_data, by = "name_long")
  
if(nrow(world_coffee_inner) < nrow(coffee_data)) {
  warning("innerjoin does not match orig data")
}

# why, will need to look at df
```

```{r}

```


```{r}
drc <- stringr::str_subset(world$name_long, "Dem*, Congo")

coffee_data$name_long{stringr::str_detect(coffee_data$name_long, by = "Congo")}

world_coffee_inner <- inner_join(world, coffee_data, by = "name_long)")
```

